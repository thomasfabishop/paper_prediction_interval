---
title: "Prediction Interval"
author: "T. Bishop"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Set Up

Start: August 2013

Load packages.

```{r}
library(geoR)
library(lattice)
library(gstat)
library(moments)
library(ggplot2)
library(raster)
library(moments)
library(randomForest)
```

# Process Data

## Import
```{r}
calib=read.table("C:\\Users\\bishop\\Dropbox (Sydney Uni)\\Research\\Projects\\Pedometrics2013\\Paper_Pred_Interval\\Data\\DecOdeh00_10.txt", header=TRUE, sep="\t", dec=".")
valid=read.table("C:\\Users\\bishop\\Dropbox (Sydney Uni)\\Research\\Projects\\Pedometrics2013\\Paper_Pred_Interval\\Data\\2010pcdata.txt", header=TRUE, sep="\t", dec=".")
```

## Fix names to match

```{r}
colnames(calib)[3]<-"clay"
colnames(valid)[4]<-"slu"
colnames(valid)[5]<-"lu"
colnames(valid)[6]<-"soil"
```

## Factors for soil, lu and slu

```{r}
calib$lu<-as.factor(calib$lu)
calib$slu<-as.factor(calib$slu)
calib$soil<-as.factor(calib$soil)

valid$lu<-as.factor(valid$lu)
valid$slu<-as.factor(valid$slu)
valid$soil<-as.factor(valid$soil)
```

# Exploratory Data Analysis

## Univariate

### Calibration set

```{r}
summary(calib$clay)
length(calib$clay)
skewness(calib$clay)
sqrt(var(calib$clay))
```

```{r}
sum.df<-data.frame (
  stats=c("mean = 31.0","median = 31.5", "std. deviation = 16.5", "observations = 97", "skewness = 0.20"),
  y=c(17.5, 16.7, 15.9, 15.1, 14.3))

#tiff('hist_calib.tiff')
ggplot(calib, aes(x=clay)) + geom_histogram(binwidth=6,colour='black',fill='grey')+
  xlab("Clay (%)") +
  ylab("Frequency") +
  theme(axis.title.y = element_text(size=20))+
  theme(axis.title.y = element_text(size=12),axis.text.y  = element_text(size=10))+
  xlim(0,85)+
  ylim(0,20)+
  theme(axis.title.x = element_text(size=12),axis.text.x  = element_text(size=10))+
  geom_text(data=sum.df,aes(x=50,y=y,label=stats),size=5, hjust=0)+
  annotate("text",x=0, y=20.0, label="Calibration dataset",size=7, hjust = 0)
#   annotate("text",x=60, y=15, label="Median = 31.5",size=4)+
#   annotate("text",x=60, y=12, label="Observations = 97",size=4)
#dev.off()
```

### Validation set

```{r}
summary(valid$clay)
length(valid$clay)
skewness(valid$clay)
sqrt(var(valid$clay))
```

```{r}
sum.df<-data.frame (
  stats=c("mean = 41.7","median = 45.2", "std. deviation = 22.5", "observations = 44",  "skewness = -0.27"),
  y=c(9, 8.6, 8.2, 7.8, 7.4))

#tiff('hist_valid.tiff')
ggplot(valid, aes(x=clay)) + geom_histogram(binwidth=6,colour='black',fill='grey')+
  xlab("Clay (%)") +
  ylab("Frequency") +
  theme(axis.title.y = element_text(size=20))+
  theme(axis.title.y = element_text(size=12),axis.text.y  = element_text(size=10))+
  xlim(0,85)+
  ylim(0,10)+
  theme(axis.title.x = element_text(size=12),axis.text.x  = element_text(size=10))+
  geom_text(data=sum.df,aes(x=50,y=y,label=stats),size=5, hjust=0)+
  annotate("text",x=0, y=10.0, label="Validation dataset",size=7, hjust = 0)
#   annotate("text",x=60, y=15, label="Median = 31.5",size=4)+
#   annotate("text",x=60, y=12, label="Observations = 97",size=4)
#dev.off()
```

## Bivariate

### Soil Classes

Re-code soil class

```{r}
calib$soil.type<-ifelse(calib$soil==1, "Vertosol", "Other")

#tiff('soil_clay.tiff')
p <- ggplot(calib, aes(x=soil.type, y=clay)) + geom_boxplot() +
  xlab("Soil Type") +
  ylab("Clay (%)")+
  theme(axis.title.y = element_text(size=20))+
  ylim(0,80)+
  theme(axis.title.y = element_text(size=12),axis.text.y  = element_text(size=10))+
  theme(axis.title.x = element_text(size=12),axis.text.x  = element_text(size=10))
p
#dev.off()
```

```{r}
plot(calib$soil,calib$clay)
plot(valid$soil,valid$clay)
tapply(calib$clay,calib$soil,length)
```

Shows some difference - a candidate

### Land Use

COULD ADD label to figures
("text",x=80, y=0, label="a",size=8)

Re-code land use

```{r}
calib$land.use<-ifelse(calib$lu==1, "Dryland cropping",
                 ifelse(calib$lu==2,"Forest",
                        ifelse(calib$lu==3, "Pasture","Irrigated")))

head(calib$lu)
```

Cropping, Forest, Pasture, Irrigated

```{r}
plot(calib$lu,calib$clay)
plot(valid$lu,valid$clay)
tapply(calib$clay,calib$lu,length)
tapply(valid$clay,valid$lu,length)
```

Shows some difference - a candidate

```{r}
#tiff('luse_clay.tiff')
p <- ggplot(calib, aes(x=land.use, y=clay)) + geom_boxplot() +
  xlab("Land Use") +
  ylab("Clay (%)")+
  theme(axis.title.y = element_text(size=20))+
  ylim(0,80)+
  theme(axis.title.y = element_text(size=12),axis.text.y  = element_text(size=10))+
  theme(axis.title.x = element_text(size=12),axis.text.x  = element_text(size=10))
p
#dev.off()
```

### PCA

```{r}
skewness(calib[,7:12])
cor(calib$clay,calib[,7:12])
cor(calib$clay,calib[,7:12],method="spearman")
```

No diff with spearman

# Linear Mixed Model

## Regression with OLS

Build prediction frame

```{r}
calib<-cbind(calib$clay,calib[,1:2],calib[,5:12])
colnames(calib)[1]<-"clay"
```

Fit model

```{r}
clay.lm<-lm(clay~soil+lu+pc1+pc2+pc3+pc4+pc5+pc6, data=calib)
summary(clay.lm)
```

Model shows some promise with adjusted r2 of 0.46.

Test model assumptions

```{r}
par(mfrow=c(2,2))
hist(rstandard(clay.lm),xlab="Standardised residuals",ylab="Frequency",main=NULL)
#skewness(clay.lm$residuals)
plot(clay.lm$fitted.values,(rstandard(clay.lm)),xlab="Fitted values",ylab="Standardised residuals")
qqnorm(rstandard(clay.lm),main=NULL,xlab="Normal quantiles",ylab="Sample quantiles")
abline(0,1)
```

Assumptions met but lets check with Wally Plots.

```{r}
library(MESS)
wallyplot(clay.lm)
qqnorm.wally <- function(x, y, ...) { qqnorm(y, ...) ; abline(a=0, b=1) }
wallyplot(clay.lm, FUN=qqnorm.wally, main="")
```



## Fit Linear Mixed Model

Create GeoR object and include OLS residuals to examine spatial structure of these to see if kriging residuals has merit.

```{r}
calib<-cbind(calib,clay.lm$residuals)
head(calib)
colnames(calib)[12] <- "residuals"
res <- as.geodata(calib,coords.col=2:3,data.col=12, covar.col=4:11)
res_var <- variog(res,option= "bin",messages = T,uvec=c(seq(0,50000,2000)))
plot(res_var, pch = 19, ylim=c(0,300),xlim=c(0,30000),col= "black", main="",xlab="distance (m)")
```

Residuals indicate short range spatail structure out to 5km so kriging of residuals shows promise.  Now create new GeoR object where response is clay.

```{r}
gcalib <- as.geodata(calib,coords.col=2:3,data.col=1, covar.col=4:11)
summary(gcalib)
plot(gcalib)
```

Define initial parameters for spatial correlation based on residual semivariogram.  Use nugget as 1st lag semivariance.  Have a few parameter combinations to find maximum likelihood and avoid local optima. First set is:

```{r}
init.par<-c(50,5000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~soil+lu+pc1+pc2+pc3+pc4+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
lmm.exp$loglik
lmm.exp$cov.pars
lmm.exp$phi
lmm.exp$nugget
```

Combination 2

```{r}
init.par<-c(25,5000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~soil+lu+pc1+pc2+pc3+pc4+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
lmm.exp$loglik
lmm.exp$cov.pars
lmm.exp$phi
lmm.exp$nugget
```

Combination 3

```{r}
init.par<-c(75,5000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~soil+lu+pc1+pc2+pc3+pc4+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
lmm.exp$loglik
lmm.exp$cov.pars
lmm.exp$phi
lmm.exp$nugget
```

Combination 4

```{r}
init.par<-c(50,10000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~soil+lu+pc1+pc2+pc3+pc4+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
lmm.exp$loglik
lmm.exp$cov.pars
lmm.exp$phi
lmm.exp$nugget
```

Combination 5

```{r}
init.par<-c(50,2000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~soil+lu+pc1+pc2+pc3+pc4+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
lmm.exp$loglik
lmm.exp$cov.pars
lmm.exp$phi
lmm.exp$nugget
```

The best combination is 5 but it is marginal.  The spatial model is best due to smaller AIC.
We plot model over residual variogram to see fit.

```{r}
plot(res_var, pch = 19, ylim=c(0,300),xlim=c(0,30000),col= "black", main="",xlab="distance (m)")
lines(lmm.exp)
```

### Variable Selection

Below is the function to perform Wald tests on the fixed effects.

```{r}
wald.test<-function(model.geostat){
fitmodel<-model.geostat
# get coefficients
coefficients <- fitmodel$beta
# get standard errors
se_error <- sqrt(diag(fitmodel$beta.var))
# get t values
t_value <- coefficients/se_error
# and probabilities
t_prob <- 2 * pt(-abs(t_value), df = (nrow(calib) -length (coefficients)))
# make pretty
coef_mat <- cbind(coefficients, se_error, t_value, t_prob)
colnames(coef_mat) <- c("Estimate", "Std.Err","t value", "Pr(>|t|)")
rownames(coef_mat) <- colnames(model.matrix(fitmodel$trend, gcalib))
return(printCoefmat(coef_mat))
}
```

We apply it to our model with all covariates.

```{r}
wald.test(lmm.exp)
```

We remove the covariate with the largest P-value which is pc2 and run again.

```{r}
init.par<-c(50,2000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~soil+lu+pc1+pc3+pc4+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
summary(lmm.exp)
wald.test(lmm.exp)
```

We remove pc1.

```{r}
init.par<-c(50,2000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~soil+lu+pc3+pc4+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
summary(lmm.exp)
wald.test(lmm.exp)
```

We remove soil.

```{r}
init.par<-c(50,2000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~lu+pc3+pc4+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
summary(lmm.exp)
wald.test(lmm.exp)
```

We remove pc3.

```{r}
init.par<-c(50,2000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~lu+pc4+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
summary(lmm.exp)
wald.test(lmm.exp)
```

We remove pc4.

```{r}
init.par<-c(50,2000)
nug.par=50
lmm.exp <- likfit(gcalib, trend= ~lu+pc5+pc6,
                  lambda=1,ini.cov.pars = init.par,fix.nugget = FALSE, nugget = nug.par, 
                  lik.method = "REML", cov.model = "exp",limits = pars.limits(phi=c(0, 50000)))
summary(lmm.exp)
wald.test(lmm.exp)
```

We now have our final model with land use and pc5 and pc6.

NOTE - likfit function not sensitive to range parameter - not sure if dataset specific or issue with library.  Kate or Liana data?

## Model Testing

Now we perform cross-validation to assess the model in terms of its predictions - point and interval estimates.

First we do without re-fitting the model for each split.

```{r}
#xv.REML <- xvalid(gcalib, model = lmm.exp, reestimate=TRUE)
xv.REML <- xvalid(gcalib, model = lmm.exp)
```

We test how well the prediction variance matches the errors.

```{r}
mean(xv.REML$std.error^2)
median(xv.REML$std.error^2)
```

We test the bias and accuracy.

```{r}
mean(xv.REML$error)
sqrt(mean(xv.REML$error^2))
```

Now do by re-fitting model parameters at each plit.

```{r}
xv.REML2 <- xvalid(gcalib, model = lmm.exp, reestimate=TRUE,messages="F")
```

We test how well the prediction variance matches the errors.

```{r}
mean(xv.REML2$std.error^2)
median(xv.REML2$std.error^2)
```

We test the bias and accuracy.

```{r}
mean(xv.REML2$error)
sqrt(mean(xv.REML2$error^2))
```

Let compare predictions, prediction variance and standardised error using both CV approaches. 

```{r}
plot(xv.REML$predicted,xv.REML2$predicted, xlab ="Predictions - CV with no re-estimation", ylab ="Predictions - CV with re-estimation")
abline(0,1)
plot(xv.REML$krige.var,xv.REML2$krige.var, xlab ="Prediction variance - CV with no re-estimation", ylab ="Prediction variance - CV with re-estimation")
abline(0,1)
plot(xv.REML$std.error,xv.REML2$std.error, xlab ="Standard. error - CV with no re-estimation", ylab ="Standard. error - CV with re-estimation")
abline(0,1)
```

The prediction variance is most impacted and the statistically the best approach is to re-estimate for each split.

Could add Lin's?.

```{r}
plot.df<-cbind(xv.REML$data,xv.REML$predicted)
plot.df<-as.data.frame(plot.df)
colnames(plot.df)[1] <- "observed"
colnames(plot.df)[2] <- "predicted"

sum.df<-data.frame (
  stats=c("RMSE = 11.47","ME = -0.08", "Mean SSPE = 1.07", "Median SSPE = 0.49", "LCCC = 0.70"),
  y=c(85.0, 80.0, 75.0, 70.0, 65.0))

#tiff('calib_lmm.tiff')
ggplot(plot.df, aes(x = observed, y = predicted)) +
  geom_point(size=2) +
  xlab("Observed - Clay (%)") +
  ylab("Predicted - Clay (%)") +
  theme(axis.title.y = element_text(size=12),axis.text.y  = element_text(size=10))+
  xlim(0,85)+
  ylim(0,85)+
  geom_abline(intercept = 0, slope = 1) +
  coord_equal()+
  geom_text(data=sum.df,aes(x=0,y=y,label=stats),size=5, hjust=0)
#dev.off()
```

## Model Validation

Now we predict at validation sites.

```{r}
fitmodel_gstat <- as.vgm.variomodel(lmm.exp)
coordinates(valid) <- ~x + y
coordinates(calib) <- ~x + y
eblup <- krige(clay~lu+pc5+pc6,
               calib,valid,model = fitmodel_gstat,debug.level=-1)

summary(eblup$var1.pred)
```

## Test Point Estimates

```{r}
mean(eblup$var1.pred-valid$clay)
sqrt(mean((eblup$var1.pred-valid$clay)^2))
```

### Plot data

```{r}
plot.df<-cbind(valid$clay,eblup$var1.pred)
plot.df<-as.data.frame(plot.df)
colnames(plot.df)[1] <- "observed"
colnames(plot.df)[2] <- "predicted"

sum.df<-data.frame (
  stats=c("RMSE = 16.44","ME = -6.68", "LCCC = 0.70"),
  y=c(85.0, 80.0, 75.0))

#tiff('calib_lmm.tiff')
ggplot(plot.df, aes(x = observed, y = predicted)) +
  geom_point(size=2) +
  xlab("Observed - Clay (%)") +
  ylab("Predicted - Clay (%)") +
  theme(axis.title.y = element_text(size=12),axis.text.y  = element_text(size=10))+
  xlim(0,85)+
  ylim(0,85)+
  geom_abline(intercept = 0, slope = 1) +
  coord_equal()+
  geom_text(data=sum.df,aes(x=0,y=y,label=stats),size=5, hjust=0)
#dev.off()
```



## Test Interval Estimates

```{r}
valid.df<-data.frame(valid,eblup$var1.pred,eblup$var1.var)
valid.df$ID<-seq(1,44,1)
```

95% interval

```{r}

sum.df<-data.frame (
  stats=c("Coverage Probability = 0.88"),
  y=c(-10))

PIl95<-valid.df$eblup.var1.pred-sqrt(valid.df$eblup.var1.var)*1.96
PIu95<-valid.df$eblup.var1.pred+sqrt(valid.df$eblup.var1.var)*1.96

valid.df$hit95<-as.numeric(valid.df$clay >= PIl95 & valid.df$clay <= PIu95)
hit95<-sum(valid.df$hit95)/44
ggplot(valid.df, aes(x = ID, y = clay)) +
  geom_point(data=valid.df[which(valid.df$hit95==1),],colour='blue',size=4) +
  geom_point(data=valid.df[which(valid.df$hit95==0),],colour='red',size=4) +
  #geom_point(size = 4) +
  ylab("Clay (%)") +
  xlab("Site") +
  #theme(axis.title.y = element_text(size=20))+
  theme(axis.title.y = element_text(size=20),axis.text.y  = element_text(size=14))+
  theme(axis.title.x = element_text(size=20),axis.text.x  = element_text(size=14))+
  geom_errorbar(aes(ymax = PIu95, ymin = PIl95))+
  geom_text(data=sum.df,aes(x=0,y=y,label=stats),size=5, hjust=0)

```


90% interval

```{r}

sum.df<-data.frame (
  stats=c("Coverage Probability = 0.82"),
  y=c(-10))

PIl95<-valid.df$eblup.var1.pred-sqrt(valid.df$eblup.var1.var)*1.645
PIu95<-valid.df$eblup.var1.pred+sqrt(valid.df$eblup.var1.var)*1.645

valid.df$hit90<-as.numeric(valid.df$clay >= PIl95 & valid.df$clay <= PIu95)
hit90<-sum(valid.df$hit90)/44
ggplot(valid.df, aes(x = ID, y = clay)) +
  geom_point(data=valid.df[which(valid.df$hit90==1),],colour='blue',size=4) +
  geom_point(data=valid.df[which(valid.df$hit90==0),],colour='red',size=4) +
  #geom_point(size = 4) +
  ylab("Clay (%)") +
  xlab("Site") +
  #theme(axis.title.y = element_text(size=20))+
  theme(axis.title.y = element_text(size=20),axis.text.y  = element_text(size=14))+
  theme(axis.title.x = element_text(size=20),axis.text.x  = element_text(size=14))+
  geom_errorbar(aes(ymax = PIu95, ymin = PIl95))+
  geom_text(data=sum.df,aes(x=0,y=y,label=stats),size=5, hjust=0)

```

66% interval

```{r}

sum.df<-data.frame (
  stats=c("Coverage Probability = 0.41"),
  y=c(-10))

PIl95<-valid.df$eblup.var1.pred-sqrt(valid.df$eblup.var1.var)*1
PIu95<-valid.df$eblup.var1.pred+sqrt(valid.df$eblup.var1.var)*1

valid.df$hit66<-as.numeric(valid.df$clay >= PIl95 & valid.df$clay <= PIu95)
hit66<-sum(valid.df$hit66)/44
ggplot(valid.df, aes(x = ID, y = clay)) +
  geom_point(data=valid.df[which(valid.df$hit66==1),],colour='blue',size=4) +
  geom_point(data=valid.df[which(valid.df$hit66==0),],colour='red',size=4) +
  #geom_point(size = 4) +
  ylab("Clay (%)") +
  xlab("Site") +
  #theme(axis.title.y = element_text(size=20))+
  theme(axis.title.y = element_text(size=20),axis.text.y  = element_text(size=14))+
  theme(axis.title.x = element_text(size=20),axis.text.x  = element_text(size=14))+
  geom_errorbar(aes(ymax = PIu95, ymin = PIl95))+
  geom_text(data=sum.df,aes(x=0,y=y,label=stats),size=5, hjust=0)

```

In talk had 95, 90, 80, 50, 20% PI - could do this.

Could plot covergage probability versus PI

# Random Forest Modelling

```{r}
rf_model <- randomForest(clay ~ soil+lu+pc1+pc2+pc3+pc4+pc5+pc6,data=calib, ntree = 500, mtry = 3,importance = TRUE,do.trace = 100, proximity=TRUE)
rf_predictV<-predict(rf_model, valid)
plot(valid$clay,rf_predictV)
mean(rf_predictV-valid$clay)
sqrt(mean((rf_predictV-valid$clay)^2))
```

Extract individual trees

```{r}
rf_predictVi<-predict(rf_model, valid,predict.all=TRUE) 
individual=rf_predictVi$individual #extract the matrix where each column contains the predictions by each tree in the forest
aggregate=rf_predictVi$aggregate  #mean of trees
```

Extract parts of tree
```{r}
aggregate=rf_predictVi$aggregate  #extract the average of predictions
agg1=aggregate[[1]]   #extract the average for a specific position
```

extract the entire Mth row corresponding to all the predictions for that position in the tree
extract quantiles for validation points

```{r}
output<-data.frame(NA,ncol=12,nrow=length(individual[,1]))

kount<-0

for (i in 1:length(individual[,1])){
    
  ind1=individual[i,]
  
  qu<-quantile(ind1, probs=c(0.025,0.05,0.1,0.25,0.4,0.5,0.6,0.75,0.9,0.95,0.975))
  ave<-mean(ind1)
  
  kount<-kount+1
  
  output[kount,1]<-qu[1]
  output[kount,2]<-qu[2]
  output[kount,3]<-qu[3]
  output[kount,4]<-qu[4]
  output[kount,5]<-qu[5]
  output[kount,6]<-qu[6]
  output[kount,7]<-qu[7]
  output[kount,8]<-qu[8]
  output[kount,9]<-qu[9]
  output[kount,10]<-qu[10]
  output[kount,11]<-qu[11]
  output[kount,12]<-ave  
  
  print(kount)
}



qout<-output

names(qout)[1:12] <- c("q2.5","q5","q10","q25","q40","q50","q60","q75","q90","q95","q97.5","ave")

head(qout)

id<-1:44

qout<-cbind(id,valid$clay,qout)

names(qout)[1:2] <- c("Site","Clay")

```

#95% PI
#Calculate hits

```{r}
qout$hit95<-as.numeric(qout$Clay >= qout$q2.5 & qout$Clay <= qout$q97.5)
sum(qout$hit95)/44
```

```{r}
ggplot(qout, aes(x = Site, y = Clay)) + 
  geom_point(data=qout[which(qout$hit95==1),],colour='blue',size=4) +
  geom_point(data=qout[which(qout$hit95==0),],colour='red',size=4) +
  #geom_point(size = 4) +
  ylab("Clay (%)") +
  xlab("Site") +
  #theme(axis.title.y = element_text(size=20))+
  theme(axis.title.y = element_text(size=20),axis.text.y  = element_text(size=14))+
  theme(axis.title.x = element_text(size=20),axis.text.x  = element_text(size=14))+
  geom_errorbar(aes(ymax = q97.5, ymin = q2.5))
```

Put in set.seed to get same RF split

Save workspace

```{r}
save.image("paper.RMD")

```
















